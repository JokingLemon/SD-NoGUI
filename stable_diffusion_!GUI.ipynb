{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JokingLemon/SD-NoGUI/blob/main/stable_diffusion_!GUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ab018ae16a9b4fc3af629f587d80458a",
            "870d95156a0348c58d8e605564fa296f",
            "5cca6e8c78c54492a6ee048ef8413224"
          ]
        },
        "id": "_u3q60di584x",
        "outputId": "fcf08a00-e49a-4bbf-d0a4-55bf9edbe325"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab018ae16a9b4fc3af629f587d80458a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title ## 1.1. Install Dependencies\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "import zipfile\n",
        "import shutil\n",
        "import importlib\n",
        "import time\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "from IPython.utils import capture\n",
        "%store -r\n",
        "\n",
        "# root_dir\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"src\")\n",
        "lora_dir = os.path.join(root_dir, \"LoRA\")\n",
        "pretrained_model = os.path.join(root_dir, \"pretrained_model\")\n",
        "vae_dir = os.path.join(root_dir, \"vae\")\n",
        "\n",
        "# repo_dir\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir, \"tools\")\n",
        "finetune_dir = os.path.join(repo_dir, \"finetune\")\n",
        "\n",
        "for store in [\n",
        "    \"root_dir\",\n",
        "    \"deps_dir\",\n",
        "    \"repo_dir\",\n",
        "    \"lora_dir\",\n",
        "    \"pretrained_model\",\n",
        "    \"vae_dir\",\n",
        "    \"accelerate_config\",\n",
        "    \"tools_dir\",\n",
        "    \"finetune_dir\"\n",
        "]:\n",
        "    with capture.capture_output() as cap:\n",
        "        %store {store}\n",
        "        del cap\n",
        "\n",
        "repo_url = \"https://github.com/JokingLemon/SD-NoGUI\"\n",
        "bitsandytes_main_py = \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py\"\n",
        "branch = \"\"  \n",
        "install_xformers = True \n",
        "mount_drive = False  # @param {type: \"boolean\"}\n",
        "verbose = False \n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "\n",
        "def clone_repo(url):\n",
        "    if not os.path.exists(repo_dir):\n",
        "        os.chdir(root_dir)\n",
        "        !git clone {url} {repo_dir}\n",
        "    else:\n",
        "        os.chdir(repo_dir)\n",
        "        !git pull origin {branch} if branch else !git pull\n",
        "\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "    !wget {'-q' if not verbose else ''} --show-progress {url}\n",
        "    with zipfile.ZipFile(name, \"r\") as deps:\n",
        "        deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "\n",
        "def install_dependencies():\n",
        "    s = getoutput('nvidia-smi')\n",
        "\n",
        "    '''if 'T4' in s:\n",
        "        !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "    '''\n",
        "    !pip {'-q' if not verbose else ''} install --upgrade -r requirements.txt\n",
        "    !pip install {'-q' if not verbose else ''} torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 torchtext==0.15.1 torchdata==0.6.0 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
        "\n",
        "    if install_xformers:\n",
        "        !pip {'-q' if not verbose else ''} install xformers==0.0.18 triton\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "    if mount_drive:\n",
        "        if not os.path.exists(\"/content/drive\"):\n",
        "            drive.mount(\"/content/drive\")\n",
        "\n",
        "    for dir in [\n",
        "        deps_dir, \n",
        "        lora_dir, \n",
        "        pretrained_model, \n",
        "        vae_dir\n",
        "    ]:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "    clone_repo(repo_url)\n",
        "\n",
        "    os.chdir(repo_dir)\n",
        "    \n",
        "    !apt -y update {'-qq' if not verbose else ''}\n",
        "    !apt install libunwind8-dev {'-qq' if not verbose else ''}\n",
        "\n",
        "    ubuntu_deps(\n",
        "        \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\",\n",
        "        \"deb-libs.zip\",\n",
        "        deps_dir,\n",
        "    )\n",
        "\n",
        "    install_dependencies()\n",
        "\n",
        "    os.environ[\"LD_PRELOAD\"] = \"libtcmalloc.so\"\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"  \n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "    cuda_path = \"/usr/local/cuda-11.8/targets/x86_64-linux/lib/\"\n",
        "    ld_library_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "    os.environ[\"LD_LIBRARY_PATH\"] = f\"{ld_library_path}:{cuda_path}\"\n",
        "\n",
        "main()\n",
        "clear_output()\n",
        " \n",
        "b=widgets.Button(description='\\u2714 Done', disabled=True, button_style='success', layout=widgets.Layout(min_width='50px'))\n",
        "display(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "HSdQSWcJNeAG",
        "outputId": "f5c565d5-fd05-4dbe-964b-4e2c29db12e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: control_v11p_sd15_canny.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.35G/1.35G [00:12<00:00, 114MB/s] \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 88>\u001b[0m:\u001b[94m96\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'b'\u001b[0m is not defined\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 88&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">96</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span> is not defined\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title # ControlNet\n",
        "from torch.hub import download_url_to_file\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "from IPython.utils import capture\n",
        "from subprocess import run\n",
        "\n",
        "Model = \"Canny\" #@param [ \"None\", \"All (21GB)\", \"Canny\", \"Depth\", \"Lineart\", \"MLSD\", \"Normal\", \"OpenPose\", \"Scribble\", \"Seg\", \"ip2p\", \"Shuffle\", \"Inpaint\", \"Softedge\", \"Lineart_Anime\", \"Tile\", \"T2iadapter_Models\"]\n",
        "\n",
        "v2_Model = \"None\" #@param [ \"None\", \"All\", \"Canny\", \"Depth\", \"HED\", \"OpenPose\", \"Scribble\"]\n",
        "\n",
        "#@markdown - Download/update ControlNet extension and its models\n",
        "\n",
        "def download(url, model_dir):\n",
        "\n",
        "    filename = os.path.basename(urlparse(url).path)\n",
        "    pth = os.path.abspath(os.path.join(model_dir, filename))\n",
        "    if not os.path.exists(pth):\n",
        "        print('Downloading: '+os.path.basename(url))\n",
        "        download_url_to_file(url, pth, hash_prefix=None, progress=True)\n",
        "    else:\n",
        "      print(f\"\u001b[1;32mThe model {filename} already exists\u001b[0m\")\n",
        "blasphemy=\"web\"+\"ui\"\n",
        "Canny='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_canny.pth'\n",
        "Depth='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth'\n",
        "Lineart='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_lineart.pth'\n",
        "MLSD='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_mlsd.pth'\n",
        "Normal='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_normalbae.pth'\n",
        "OpenPose='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_openpose.pth'\n",
        "Scribble='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_scribble.pth'\n",
        "Seg='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_seg.pth'\n",
        "ip2p='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11e_sd15_ip2p.pth'\n",
        "Shuffle='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11e_sd15_shuffle.pth'\n",
        "Inpaint='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_inpaint.pth'\n",
        "Softedge='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_softedge.pth'\n",
        "Lineart_Anime='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15s2_lineart_anime.pth'\n",
        "Tile='https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1e_sd15_tile.pth'\n",
        "\n",
        "\n",
        "mdldir='/content/src/ControlNet/models'\n",
        "for filename in os.listdir(mdldir):\n",
        "  if \"_sd14v1\" in filename:\n",
        "    renamed = re.sub(\"_sd14v1\", \"-fp16\", filename)\n",
        "    os.rename(os.path.join(mdldir, filename), os.path.join(mdldir, renamed))\n",
        "\n",
        "!wget -q -O CN_models.txt https://github.com/TheLastBen/fast-stable-diffusion/raw/main/AUTOMATIC1111_files/CN_models.txt\n",
        "!wget -q -O CN_models_v2.txt https://github.com/TheLastBen/fast-stable-diffusion/raw/main/AUTOMATIC1111_files/CN_models_v2.txt\n",
        "\n",
        "with open(\"CN_models.txt\", 'r') as f:\n",
        "  mdllnk = f.read().splitlines()\n",
        "with open(\"CN_models_v2.txt\", 'r') as d:\n",
        "  mdllnk_v2 = d.read().splitlines()\n",
        "\n",
        "!rm CN_models.txt CN_models_v2.txt\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  cfgnames=[os.path.basename(url).split('.')[0]+'.yaml' for url in mdllnk_v2]\n",
        "  %cd /content/src/ControlNet/models\n",
        "  for name in cfgnames:\n",
        "      run(['cp', 'cldm_v21.yaml', name])\n",
        "  %cd /content\n",
        "\n",
        "if Model == \"All (21GB)\": \n",
        "  for lnk in mdllnk:\n",
        "      download(lnk, mdldir)\n",
        "  clear_output()\n",
        "\n",
        "elif Model == \"T2iadapter_Models\":\n",
        "  mdllnk=list(filter(lambda x: 't2i' in x, mdllnk))\n",
        "  for lnk in mdllnk:\n",
        "      download(lnk, mdldir)\n",
        "  clear_output()\n",
        "\n",
        "elif Model == \"None\":\n",
        "    pass\n",
        "    clear_output()\n",
        "\n",
        "else:\n",
        "  download(globals()[Model], mdldir)\n",
        "  \n",
        "\n",
        "Canny='https://huggingface.co/thibaud/controlnet-sd21/resolve/main/control_v11p_sd21_canny.safetensors'\n",
        "Depth='https://huggingface.co/thibaud/controlnet-sd21/resolve/main/control_v11p_sd21_depth.safetensors'\n",
        "HED='https://huggingface.co/thibaud/controlnet-sd21/resolve/main/control_v11p_sd21_hed.safetensors'\n",
        "OpenPose='https://huggingface.co/thibaud/controlnet-sd21/resolve/main/control_v11p_sd21_openposev2.safetensors'\n",
        "Scribble='https://huggingface.co/thibaud/controlnet-sd21/resolve/main/control_v11p_sd21_scribble.safetensors'\n",
        "\n",
        "if v2_Model == \"All\":\n",
        "  for lnk_v2 in mdllnk_v2:\n",
        "      download(lnk_v2, mdldir)\n",
        "  \n",
        "  display(b)\n",
        "\n",
        "elif v2_Model == \"None\":\n",
        "    pass\n",
        "    display(b)\n",
        "\n",
        "else:\n",
        "  download(globals()[v2_Model], mdldir)\n",
        "  display(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jch4cfKBSdlE",
        "outputId": "3b7ec602-9bdf-4d46-f961-1fff4b4a57bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/src\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/src\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VT6NLv-2u6q",
        "outputId": "5e428d46-7bf1-479f-e644-3b55cf3cbadb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " *** Download Progress Summary as of Sun May 28 13:45:02 2023 *** \n",
            "=\n",
            "[#c7e0fd 2.1GiB/3.9GiB(53%) CN:16 DL:209MiB ETA:9s]\n",
            "FILE: /content/pretrained_model/Stable-Diffusion-v1-5.safetensors\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sun May 28 13:45:13 2023 *** \n",
            "=\n",
            "[#c7e0fd 3.7GiB/3.9GiB(94%) CN:16 DL:145MiB ETA:1s]\n",
            "FILE: /content/pretrained_model/Stable-Diffusion-v1-5.safetensors\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "c7e0fd|\u001b[1;32mOK\u001b[0m  |   174MiB/s|/content/pretrained_model/Stable-Diffusion-v1-5.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# @title ## 2.1. Download Available Model\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "models = {\n",
        "    \"Animefull-final-pruned\": \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt\",\n",
        "    \"Anything-v3-1\": \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors\",\n",
        "    \"AnyLoRA\": \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_bakedVae_fp16_NOTpruned.safetensors\",\n",
        "    \"AnimePastelDream\": \"https://huggingface.co/Lykon/AnimePastelDream/resolve/main/AnimePastelDream_Soft_noVae_fp16.safetensors\",\n",
        "    \"Chillout-mix\": \"https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/chillout_mix-pruned.safetensors\",\n",
        "    \"OpenJourney-v4\": \"https://huggingface.co/prompthero/openjourney-v4/resolve/main/openjourney-v4.ckpt\",\n",
        "    \"Stable-Diffusion-v1-5\": \"https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/stable_diffusion_1_5-pruned.safetensors\",\n",
        "}\n",
        "\n",
        "v2_models = {\n",
        "    \"stable-diffusion-2-1-base\": \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors\",\n",
        "    \"stable-diffusion-2-1-768v\": \"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors\",\n",
        "    \"plat-diffusion-v1-3-1\": \"https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors\",\n",
        "    \"replicant-v1\": \"https://huggingface.co/gsdf/Replicant-V1.0/resolve/main/Replicant-V1.0.safetensors\",\n",
        "    \"illuminati-diffusion-v1-0\": \"https://huggingface.co/IlluminatiAI/Illuminati_Diffusion_v1.0/resolve/main/illuminati_diffusion_v1.0.safetensors\",\n",
        "    \"illuminati-diffusion-v1-1\": \"https://huggingface.co/4eJIoBek/Illuminati-Diffusion-v1-1/resolve/main/illuminatiDiffusionV1_v11.safetensors\",\n",
        "    \"waifu-diffusion-1-4-anime-e2\": \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt\",\n",
        "    \"waifu-diffusion-1-5-e2\": \"https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp32.safetensors\",\n",
        "    \"waifu-diffusion-1-5-e2-aesthetic\": \"https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-aesthetic-fp32.safetensors\",\n",
        "}\n",
        "\n",
        "installModels = []\n",
        "installv2Models = []\n",
        "\n",
        "# @markdown ### SD1.x model\n",
        "model_name = \"Stable-Diffusion-v1-5\"  # @param [\"\", \"Animefull-final-pruned\", \"Anything-v3-1\", \"AnyLoRA\", \"AnimePastelDream\", \"Chillout-mix\", \"OpenJourney-v4\", \"Stable-Diffusion-v1-5\"]\n",
        "# @markdown ### SD2.x model\n",
        "v2_model_name = \"\"  # @param [\"\", \"stable-diffusion-2-1-base\", \"stable-diffusion-2-1-768v\", \"plat-diffusion-v1-3-1\", \"replicant-v1\", \"illuminati-diffusion-v1-0\", \"illuminati-diffusion-v1-1\", \"waifu-diffusion-1-4-anime-e2\", \"waifu-diffusion-1-5-e2\", \"waifu-diffusion-1-5-e2-aesthetic\"]\n",
        "\n",
        "if model_name:\n",
        "    model_url = models.get(model_name)\n",
        "    if model_url:\n",
        "        installModels.append((model_name, model_url))\n",
        "\n",
        "if v2_model_name:\n",
        "    v2_model_url = v2_models.get(v2_model_name)\n",
        "    if v2_model_url:\n",
        "        installv2Models.append((v2_model_name, v2_model_url))\n",
        "\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "    ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
        "\n",
        "    hf_token = \"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\"\n",
        "    user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {pretrained_model} -o {checkpoint_name}.{ext} \"{url}\"\n",
        "\n",
        "\n",
        "def install_checkpoint():\n",
        "    for model in installModels:\n",
        "        install(model[0], model[1])\n",
        "    for v2model in installv2Models:\n",
        "        install(v2model[0], v2model[1])\n",
        "\n",
        "\n",
        "install_checkpoint()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lbd19qkcVt_g"
      },
      "outputs": [],
      "source": [
        "# @title ## 2.2. Download Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3LWn6GzNQ4j5"
      },
      "outputs": [],
      "source": [
        "# @markdown Add model Url to modelUrlList\n",
        "\n",
        "modelUrlsList = [\n",
        "    \"https://huggingface.co/SG161222/Realistic_Vision_V2.0/resolve/main/Realistic_Vision_V2.0-fp16-no-ema.safetensors\",\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "              ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqOClvjF_t4I",
        "outputId": "61807ec6-6c28-4a53-9d78-ccdf30712cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "c510cd|\u001b[1;32mOK\u001b[0m  |   246MiB/s|/content/pretrained_model/Realistic_Vision_V2.0-fp16-no-ema.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# @title ## 2.3. Begin Download \n",
        "import os\n",
        "\n",
        "os.chdir(root_dir)\n",
        "def install(url):\n",
        "    base_name = os.path.basename(url)\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        os.chdir(pretrained_model)\n",
        "        !gdown --fuzzy {url}\n",
        "    elif \"huggingface.co\" in url:\n",
        "        if \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        # @markdown Change this part with your own huggingface token if you need to download your private model\n",
        "        hf_token = \"\"  # @param {type:\"string\"}\n",
        "        user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 32 -d {pretrained_model} -o {base_name} {url}\n",
        "    else:\n",
        "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 32 -d {pretrained_model} {url}\n",
        "\n",
        "for modelUrls in modelUrlsList:\n",
        "    urls = modelUrls.split(\",\")\n",
        "    for url in urls:\n",
        "        install(url.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qrY4KtfL6Dqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a185de-26bd-4326-ee6c-b62e94034c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\r\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "c408cb|\u001b[1;32mOK\u001b[0m  |   264MiB/s|/content/vae/stablediffusion.vae.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# @title ## 2.4. Download Available VAE (Optional)\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "vaes = {\n",
        "    \"none\": \"\",\n",
        "    \"anime.vae.pt\": \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\",\n",
        "    \"waifudiffusion.vae.pt\": \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\",\n",
        "    \"stablediffusion.vae.pt\": \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\",\n",
        "}\n",
        "install_vaes = []\n",
        "\n",
        "# @markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vae_name = \"stablediffusion.vae.pt\"  # @param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "if vae_name in vaes:\n",
        "    vae_url = vaes[vae_name]\n",
        "    if vae_url:\n",
        "        install_vaes.append((vae_name, vae_url))\n",
        "\n",
        "\n",
        "def install(vae_name, url):\n",
        "    hf_token = \"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\"\n",
        "    user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {vae_dir} -o {vae_name} \"{url}\"\n",
        "\n",
        "\n",
        "def install_vae():\n",
        "    for vae in install_vaes:\n",
        "        install(vae[0], vae[1])\n",
        "\n",
        "\n",
        "install_vae()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rjq6lcwagjIB"
      },
      "outputs": [],
      "source": [
        "# @title ## 3.1. Download Lora's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N9Gu6GS-BNvu"
      },
      "outputs": [],
      "source": [
        "# @markdown Add Lora Url to LoraUrlList\n",
        "\n",
        "LoraUrlsList = [\n",
        "    \"https://civitai.com/api/download/models/63006\",\n",
        "    \"https://civitai.com/api/download/models/18445\",\n",
        "    \"https://civitai.com/api/download/models/62833\",\n",
        "    \"https://civitai.com/api/download/models/16576\"\n",
        "\n",
        "\n",
        "    \n",
        "              ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOkYmpmwBia-",
        "outputId": "520b155a-eefc-4bc9-cacf-b4ec226361e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "fb9a5e|\u001b[1;32mOK\u001b[0m  |    36MiB/s|/content/LoRA//LowRA.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "8b17ba|\u001b[1;32mOK\u001b[0m  |   8.7MiB/s|/content/LoRA//epiNoiseoffset_v2-pynoise.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "af62c0|\u001b[1;32mOK\u001b[0m  |    23MiB/s|/content/LoRA//add_detail.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "d65b25|\u001b[1;32mOK\u001b[0m  |    34MiB/s|/content/LoRA//epi_noiseoffset2.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# @title ## 3.2. Begin Lora Download \n",
        "import os\n",
        "\n",
        "lora_dir=\"/content/LoRA/\"\n",
        "def install(url):\n",
        "    base_name = os.path.basename(url)\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        os.chdir(lora_dir)\n",
        "        !gdown --fuzzy {url}\n",
        "    elif \"huggingface.co\" in url:\n",
        "        if \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        # @markdown Change this part with your own huggingface token if you need to download your private model\n",
        "        hf_token = \"\"  # @param {type:\"string\"}\n",
        "        user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 32 -d {lora_dir} -o {base_name} {url}\n",
        "    else:\n",
        "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 32 -d {lora_dir} {url}\n",
        "\n",
        "for modelUrls in LoraUrlsList:\n",
        "    urls = modelUrls.split(\",\")\n",
        "    for url in urls:\n",
        "        install(url.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF6LaieDHXVN",
        "outputId": "2a6ec663-b428-434c-c099-46064137283c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load StableDiffusion checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = cls(wrap_storage=untyped_storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "prepare tokenizer\n",
            "Model Loaded\n"
          ]
        }
      ],
      "source": [
        "# @title ## 4.1 Load Base model and Lora\n",
        "# @markdown Enter model path and vae path\n",
        "import torch\n",
        "import os\n",
        "import src.gen_img_diffusers as gen\n",
        "import src.model_util as model_util\n",
        "import src.util as util\n",
        "import argparse\n",
        "import importlib\n",
        "cnets=[]\n",
        "model_p = \"/content/pretrained_model/Realistic_Vision_V2.0-fp16-no-ema.safetensors\"  # @param {type: \"string\"}\n",
        "vae_p=\"\"  # @param {type: \"string\"}\n",
        "os.environ[\"LD_PRELOAD\"] = \"libtcmalloc.so\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"  \n",
        "os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "cuda_path = \"/usr/local/cuda-11.8/targets/x86_64-linux/lib/\"\n",
        "ld_library_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = f\"{ld_library_path}:{cuda_path}\"\n",
        "\n",
        "device=args=None\n",
        "networks = []\n",
        "network_default_muls = []\n",
        "network_muls=[]\n",
        "vae=text_encoder=tokenizer=unet=None\n",
        "se=set()\n",
        "def loadmodel(model_path,vae_path):\n",
        "\tglobal args,device,networks,network_default_muls,vae,text_encoder,tokenizer,unet,network_muls\n",
        "\targsD={\n",
        "\t\t\t\"v2\" : False ,\n",
        "\t\t\t\"v_parameterization\" : False ,\n",
        "\t\t\t\"from_file\" : None ,\n",
        "\t\t\t\"interactive\" : False ,\n",
        "\t\t\t\"no_preview\" : False ,\n",
        "\t\t\t\"image_path\" : None ,\n",
        "\t\t\t\"mask_path\" : None ,\n",
        "\t\t\t\"strength\" : None ,\n",
        "\t\t\t\"sequential_file_name\" : False ,\n",
        "\t\t\t\"use_original_file_name\" : False ,\n",
        "\t\t\t\"n_iter\" : 1 ,\n",
        "\t\t\t\"batch_size\" : 1 ,\n",
        "\t\t\t\"vae_batch_size\" : None ,\n",
        "\t\t\t\"ckpt\" : model_path,\n",
        "\t\t\t\"vae_path\" : vae_path ,\n",
        "\t\t\t\"tokenizer_cache_dir\" : None ,\n",
        "\t\t\t\"seed\" : None ,\n",
        "\t\t\t\"iter_same_seed\" : False ,\n",
        "\t\t\t\"fp16\" : True ,\n",
        "\t\t\t\"bf16\" : False ,\n",
        "\t\t\t\"xformers\" : True ,\n",
        "\t\t\t\"diffusers_xformers\" : False ,\n",
        "\t\t\t\"opt_channels_last\" : False ,\n",
        "\t\t\t\"network_show_meta\" : False ,\n",
        "\t\t\t\"network_merge\" : False ,\n",
        "\t\t\t\"textual_inversion_embeddings\" : None ,\n",
        "\t\t\t\"XTI_embeddings\" : None ,\n",
        "\t\t\t\"max_embeddings_multiples\" : 3 ,\n",
        "\t\t\t\"clip_guidance_scale\" : 0.0 ,\n",
        "\t\t\t\"clip_image_guidance_scale\" : 0.0 ,\n",
        "\t\t\t\"vgg16_guidance_scale\" : 0.0 ,\n",
        "\t\t\t\"vgg16_guidance_layer\" : 20 ,\n",
        "\t\t\t\"guide_image_path\" : None ,\n",
        "\t\t\t\"highres_fix_scale\" : None ,\n",
        "\t\t\t\"highres_fix_steps\" : 28 ,\n",
        "\t\t\t\"highres_fix_save_1st\" : False ,\n",
        "\t\t\t\"highres_fix_latents_upscaling\" : False ,\n",
        "\t\t\t\"negative_scale\" : None ,\n",
        "\t\t\t\"control_net_models\" : None ,\n",
        "\t\t\t\"control_net_preps\" : None ,\n",
        "\t\t\t\"control_net_weights\" : None ,\n",
        "\t\t\t\"control_net_ratios\" : None ,\n",
        "\t\t\t\"network_module\": [],\n",
        "\t\t\t\"network_weight\": [],\n",
        "\t\t\t\"network_mul\": [],\n",
        "\t}\n",
        "\n",
        "\n",
        "\targs = argparse.Namespace(**argsD)\n",
        "\tCLIP_MODEL_PATH = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n",
        "\tdtype = torch.float16\n",
        "\n",
        "\ttry:\n",
        "\t\tdel text_encoder\n",
        "\texcept:\n",
        "\t\tNone\n",
        "\ttry:\n",
        "\t\tdel vae\n",
        "\texcept:\n",
        "\t\tNone\n",
        "\n",
        "\ttry:\n",
        "\t\tdel unet\n",
        "\texcept:\n",
        "\t\tNone\n",
        "\ttry:\n",
        "\t\tdel tokenizer\n",
        "\texcept:\n",
        "\t\tNone\n",
        "\timport gc\n",
        "\tgc.collect()\n",
        "\ttorch.cuda.empty_cache()\n",
        "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # \"mps\"を考量してない\n",
        "\n",
        "\tprint(\"load StableDiffusion checkpoint\")\n",
        "\ttext_encoder, vae, unet = model_util.load_models_from_stable_diffusion_checkpoint(False,args.ckpt)\n",
        "\tif args.vae_path!= \"\":\n",
        "\t\tdel vae\n",
        "\t\tvae = model_util.load_vae(args.vae_path, dtype)\n",
        "\t\tprint(\"additional VAE loaded\")\n",
        "\n",
        "\tgen.replace_unet_modules(unet, False, False)\n",
        "\ttokenizer = util.load_tokenizer(args)\n",
        "\n",
        "\tvae.to(dtype).to(device)\n",
        "\ttext_encoder.to(dtype).to(device)\n",
        "\tunet.to(dtype).to(device)\n",
        "\n",
        "\tprint(\"Model Loaded\")\n",
        " \n",
        "loadmodel(model_p,vae_p)\n",
        "import time\n",
        "import os\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "def apply_lora(pipe, lora_path, weight:float = 1.0):\n",
        "      from safetensors.torch import load_file\n",
        "      from src.networks.lora import create_network_from_weights\n",
        "      import torch\n",
        "      \n",
        "      vae = pipe.vae\n",
        "      text_encoder = pipe.text_encoder\n",
        "      unet = pipe.unet\n",
        "\n",
        "      sd = load_file(lora_path)\n",
        "      lora_network, sd = create_network_from_weights(weight, None, vae, text_encoder, unet, sd,for_inference=True)\n",
        "      lora_network.apply_to(text_encoder, unet)\n",
        "      lora_network.load_state_dict(sd)\n",
        "      lora_network.to(\"cuda\", dtype=torch.float16)\n",
        "      del lora_network,sd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "x7WeuhiO-7Px"
      },
      "outputs": [],
      "source": [
        "# @title Controlnet utils\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/src/ControlNet/\")\n",
        "sys.path.append(\"/content/src/\")\n",
        "from src.tools import original_control_net\n",
        "from src.tools.original_control_net import ControlNetInfo\n",
        "import os\n",
        "import diffusers\n",
        "from scripts.processor import canny,OpenposeModel\n",
        "from scripts import processor\n",
        "OpenPosemodel=None\n",
        "Cannymodel=None\n",
        "os.makedirs(\"/content/CNimg\",exist_ok=True)\n",
        "def getOpenPose(weight,ratio, include_face=False,no_prep=True):\n",
        "\n",
        "  global OpenPosemodel\n",
        "  if OpenPosemodel is None:\n",
        "    prep_type = \"canny\"\n",
        "    ratio = ratio\n",
        "    ctrl_unet, ctrl_net = original_control_net.load_control_net(args.v2, unet, \"/content/src/ControlNet/models/control_v11p_sd15_openpose.pth\")\n",
        "    OpenPosemodel=(ctrl_unet, ctrl_net)\n",
        "  OP=OpenposeModel()\n",
        "  run=OP.run_model\n",
        "  def prep_(img,res):\n",
        "    return run(img=img,include_body=True,include_hand=True,include_face=True,res=res)\n",
        "  def prep1_(img,res):\n",
        "    return run(img=img,include_body=True,include_hand=True,include_face=False,res=res)\n",
        "  if include_face:\n",
        "    prep=prep_\n",
        "  else:\n",
        "    prep=prep1_\n",
        "  if no_prep:\n",
        "    prep=None\n",
        "  ctrl_unet, ctrl_net = OpenPosemodel\n",
        "  return ControlNetInfo(ctrl_unet, ctrl_net, prep, weight, ratio)\n",
        "def getCanny(weight,ThrsA,ThrsB,ratio):\n",
        "  global Cannymodel\n",
        "  if Cannymodel is None:\n",
        "    ctrl_unet, ctrl_net = original_control_net.load_control_net(args.v2, unet, \"/content/src/ControlNet/models/control_v11p_sd15_canny.pth\")\n",
        "    Cannymodel=(ctrl_unet, ctrl_net)\n",
        "  ctrl_unet, ctrl_net= Cannymodel\n",
        "  def prep_(img,res):\n",
        "    return canny(img, res=res, thr_a=ThrsA, thr_b=ThrsB,)\n",
        "  prep = prep_\n",
        "  return ControlNetInfo(ctrl_unet, ctrl_net, prep, weight, ratio)\n",
        "  \n",
        "clear_control_net_to_clear_memory =False # @param {type:\"boolean\"}\n",
        "if clear_control_net_to_clear_memory:\n",
        "  del Cannymodel\n",
        "  del OpenPosemodel\n",
        "  Cannymodel=None\n",
        "  OpenPosemodel=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n7drCIf20j5C"
      },
      "outputs": [],
      "source": [
        "# @title 4.2 (Optional) Configure Lora \n",
        "# @markdown Enter Lora path and weight in the dictionary as path:weight pair\n",
        "Lora={\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VXRQYB0qEJGE"
      },
      "outputs": [],
      "source": [
        "# @title 4.2 Image Generation \n",
        "sampler = \"dpmsolver++K\"  # @param [\"ddim\",\"uniPC\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\",\"dpmsolver++K\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "output_dir= \"/content/tmp/\" #@param {type: \"string\"}\n",
        "clip_skip =1 # @param {type: \"slider\", min:1 , max:13}\n",
        "nimg=1  # @param {type: \"integer\"}\n",
        "prompt=\"nature background, dark fantasy, (majestic mountains in the background:1.2), ( dimly lit fantasy forrest,warm lightings:1.2),(river), (dark starry night sky),   realistic, ultra realistic, masterpiece, photo, night\" #@param {type: \"string\"}\n",
        "negative_prompt = \"bad art, 3d,  \" #@param {type: \"string\"}\n",
        "import copy\n",
        "width = 512 #@param {type: \"slider\", min: 0, max: 2016, step:32}\n",
        "height = 768 #@param {type: \"slider\", min: 0, max: 2016, step:32}\n",
        "scale = 9 # @param {type: \"slider\", min: 1, max: 40, step:0.5}            \n",
        "steps = 25 #@param {type: \"slider\", min: 0, max:100, step:5}\n",
        "seed = -1 #@param {type : \"integer\"}\n",
        "img2img =False #@param {type:\"boolean\"}\n",
        "strength = 0.2 #@param {type:\"slider\",min:0.1, max:1, step:0.05}\n",
        "p=''\n",
        "if img2img:\n",
        "  p='/content/drive/MyDrive/Upres/im_20230601102922_175650545.png' #@param {type:\"string\"}\n",
        "maskp=''\n",
        "inpainting =False #@param {type:\"boolean\"}\n",
        "if inpainting:\n",
        "  maskp='/content/mask.jpeg' #@param {type:\"string\"}\n",
        "\n",
        "args.sampler=sampler\n",
        "args.outdir=output_dir\n",
        "\n",
        "# @markdown ##ControlNet\n",
        "\n",
        "guide_img_path=\"/content/tmp//im_20230601110056_113244833.png\" # @param {type:\"string\"}\n",
        "ratio= 1  # @param{type:\"slider\",min:0,max:1,step:0.05}\n",
        "\n",
        "OpenPose=False # @param {type:\"boolean\"}\n",
        "OpenPoseStrength = 0.7 # @param {type :\"slider\",min:0,max:1.5,step:0.05}\n",
        "includeface= False # @param {type:\"boolean\"}\n",
        "noprep=False #@param {type:\"boolean\"}\n",
        "Canny=True # @param {type:\"boolean\"}\n",
        "CannyStrength = 1 # @param {type :\"number\"}\n",
        "ThrsA = 110 # @param{type:\"slider\",min:0,max:200,step:5}\n",
        "ThrsB = 175 # @param{type:\"slider\",min:0,max:200,step:5}\n",
        "\n",
        "import PIL\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "controlnets=None\n",
        "import cv2\n",
        "import diffusers\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDPMScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    PNDMScheduler,\n",
        "    DDIMScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    UNet2DConditionModel,\n",
        "    StableDiffusionPipeline,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "\n",
        "sched_init_args = {}\n",
        "scheduler_num_noises_per_step = 1\n",
        "if args.sampler == \"ddim\":\n",
        "    scheduler_cls = DDIMScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_ddim\n",
        "elif args.sampler ==\"uniPC\":\n",
        "    scheduler_cls= UniPCMultistepScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_unipc_multistep\n",
        "elif args.sampler == \"ddpm\":  \n",
        "    scheduler_cls = DDPMScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_ddpm\n",
        "elif args.sampler == \"pndm\":\n",
        "    scheduler_cls = PNDMScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_pndm\n",
        "elif args.sampler == \"lms\" or args.sampler == \"k_lms\":\n",
        "    scheduler_cls = LMSDiscreteScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_lms_discrete\n",
        "elif args.sampler == \"euler\" or args.sampler == \"k_euler\":\n",
        "    scheduler_cls = EulerDiscreteScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_euler_discrete\n",
        "elif args.sampler == \"euler_a\" or args.sampler == \"k_euler_a\":\n",
        "    scheduler_cls = EulerAncestralDiscreteScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_euler_ancestral_discrete\n",
        "elif args.sampler == \"dpmsolver++K\":\n",
        "    scheduler_cls = DPMSolverMultistepScheduler\n",
        "    sched_init_args[\"algorithm_type\"] = \"dpmsolver++\"\n",
        "    sched_init_args[\"use_karras_sigmas\"] = True\n",
        "    scheduler_module = diffusers.schedulers.scheduling_dpmsolver_multistep\n",
        "elif args.sampler == \"dpmsingle\":\n",
        "    scheduler_cls = DPMSolverSinglestepScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_dpmsolver_singlestep\n",
        "elif args.sampler == \"heun\":\n",
        "    scheduler_cls = HeunDiscreteScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_heun_discrete\n",
        "elif args.sampler == \"dpm_2\" or args.sampler == \"k_dpm_2\":\n",
        "    scheduler_cls = KDPM2DiscreteScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_discrete\n",
        "elif args.sampler == \"dpm_2_a\" or args.sampler == \"k_dpm_2_a\":\n",
        "    scheduler_cls = KDPM2AncestralDiscreteScheduler\n",
        "    scheduler_module = diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete\n",
        "    scheduler_num_noises_per_step = 2\n",
        "\n",
        "\n",
        "noise_manager = gen.NoiseManager()\n",
        "if scheduler_module is not None:\n",
        "    scheduler_module.torch = gen.TorchRandReplacer(noise_manager)\n",
        "SCHEDULER_LINEAR_START = 0.00085\n",
        "SCHEDULER_LINEAR_END = 0.0120\n",
        "SCHEDULER_TIMESTEPS = 1000\n",
        "SCHEDLER_SCHEDULE = \"scaled_linear\"\n",
        "\n",
        "scheduler = scheduler_cls(\n",
        "    num_train_timesteps=SCHEDULER_TIMESTEPS,\n",
        "    beta_start=SCHEDULER_LINEAR_START,\n",
        "    beta_end=SCHEDULER_LINEAR_END,\n",
        "    beta_schedule=SCHEDLER_SCHEDULE,\n",
        "    **sched_init_args,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "import math\n",
        "from PIL import Image\n",
        "gen_iter=1\n",
        "def gen_img(nimg,prompt,negative_prompt,width,height,scale,steps,seed=0,strength=0.1,p='',clip_skip=1,output_dir=None,guide_img_path=\"\",maskp=\"\",\n",
        "            OpenPose=False,Canny=False,OpenPoseStrength=1,CannyStrength=1,ThrsA=50,ThrsB=100,ratio=1,includeface=False,noprep=False):\n",
        "  if output_dir is None:\n",
        "    output_dir=\"/content/tmp/\"\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  vae1=copy.deepcopy(vae)\n",
        "  text_encoder1=copy.deepcopy(text_encoder)\n",
        "  tokenizer1=copy.deepcopy(tokenizer)\n",
        "  unet1=copy.deepcopy(unet)\n",
        "  seeds=[seed] if seed>0 else None\n",
        "  pipe = gen.PipelineLike(\n",
        "          device,\n",
        "          vae1,\n",
        "          text_encoder1,\n",
        "          tokenizer1,\n",
        "          unet1,\n",
        "          scheduler,\n",
        "          clip_skip,\n",
        "          None,\n",
        "          args.clip_guidance_scale,\n",
        "          args.clip_image_guidance_scale,\n",
        "          None,\n",
        "          args.vgg16_guidance_scale,\n",
        "          args.vgg16_guidance_layer,\n",
        "      )\n",
        "  del vae1,text_encoder1,tokenizer1,unet1\n",
        "\n",
        "  for x in Lora:\n",
        "    apply_lora(pipe,x,Lora[x])\n",
        "\n",
        "  if seeds is not None:\n",
        "    if len(seeds) < nimg:\n",
        "      seeds = [seed+i for i in range(nimg)]\n",
        "    seeds = seeds[: nimg]\n",
        "  else:\n",
        "    seeds = [random.randint(0, 0x7FFFFFFF) for _ in range(nimg)]\n",
        "      \n",
        "  noise_shape = (4, height // 8, width // 8)\n",
        "\n",
        "  init_image = mask_image = guide_image = i2i_noises = None\n",
        "  if maskp!='' and p!='':\n",
        "    mask_image=Image.open(maskp)\n",
        "    mask_image=mask_image.resize((width,height), PIL.Image.LANCZOS)\n",
        "  if p!='':\n",
        "    init_image = Image.open(p)\n",
        "    init_image=init_image.resize((width,height), PIL.Image.LANCZOS)\n",
        "    steps=int(steps/strength)\n",
        "    i2i_noises=torch.zeros((1, *noise_shape), device=\"cuda\", dtype=torch.float16)\n",
        "  controlnets=None\n",
        "  \n",
        "  if guide_img_path!=\"\" and (OpenPose or Canny):\n",
        "    guide_image=Image.open(guide_img_path)\n",
        "    guide_image=guide_image.resize((width,height))\n",
        "    controlnets=[]\n",
        "  if Canny:\n",
        "    controlnets.append(getCanny(CannyStrength,ThrsA,ThrsB,ratio))\n",
        "  if OpenPose:\n",
        "    controlnets.append(getOpenPose(OpenPoseStrength,ratio=ratio,include_face=includeface,no_prep=noprep))\n",
        "  pipe.set_control_nets(controlnets)\n",
        "\n",
        "  noises = [\n",
        "                torch.zeros((1, *noise_shape), device=\"cuda\", dtype=torch.float16)\n",
        "                for _ in range(steps * scheduler_num_noises_per_step)\n",
        "            ]\n",
        "\n",
        "\n",
        "  for seed in seeds:\n",
        "    torch.manual_seed(seed)\n",
        "    if i2i_noises is not None:  # img2img noise\n",
        "        i2i_noises[0] = torch.randn(noise_shape, device=device, dtype=torch.float16)\n",
        "\n",
        "    for j in range(steps * scheduler_num_noises_per_step):\n",
        "        noises[j][0] = torch.randn(noise_shape, device=device, dtype=torch.float16)\n",
        "\n",
        "    noise_manager.reset_sampler_noises(noises)\n",
        "\n",
        "    images = pipe(\n",
        "                  prompt,\n",
        "                  negative_prompt,\n",
        "                  init_image,\n",
        "                  mask_image,\n",
        "                  height,\n",
        "                  width,\n",
        "                  \n",
        "                  steps,\n",
        "                  scale,\n",
        "                  None,\n",
        "                  strength,\n",
        "                  latents=None,\n",
        "                  output_type=\"pil\",\n",
        "                  max_embeddings_multiples=3,\n",
        "                  img2img_noise=i2i_noises,\n",
        "                  vae_batch_size=None,\n",
        "                  return_latents=False,\n",
        "                  clip_prompts=None,\n",
        "                  clip_guide_images=[guide_image],\n",
        "              )[0]\n",
        "    image=images[0]\n",
        "    ts_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
        "    fln = f\"im_{ts_str}_{seed}.png\"\n",
        "\n",
        "    metadata = PngInfo()\n",
        "    metadata.add_text(\"prompt\", prompt)\n",
        "    metadata.add_text(\"seed\", str(seed))\n",
        "    metadata.add_text(\"steps\", str(steps))\n",
        "    metadata.add_text(\"scale\", str(scale))\n",
        "    metadata.add_text(\"model\",str(model_p.split('/')[-1]))\n",
        "    metadata.add_text(\"negative-prompt\", negative_prompt)\n",
        "    if len(Lora)>0:\n",
        "        metadata.add_text(\"Lora\", str(Lora))\n",
        "    image.save(os.path.join(output_dir, fln), pnginfo=metadata)\n",
        "\n",
        "  del pipe\n",
        "\n",
        "  if controlnets:\n",
        "    for x in controlnets:\n",
        "      try:\n",
        "        for y in x:\n",
        "          del y\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      del x\n",
        "gen_img(nimg,prompt,negative_prompt,width,height,scale,steps,seed,strength,p,clip_skip,output_dir,guide_img_path,maskp,\n",
        "            OpenPose,Canny,OpenPoseStrength,CannyStrength,ThrsA,ThrsB,ratio,includeface,noprep)\n",
        "print(\"done!\")\n",
        "import PIL\n",
        "for x in sorted(os.listdir(output_dir)):\n",
        "  if x in se:\n",
        "    continue\n",
        "  else:\n",
        "    se.add(x)\n",
        "    x=output_dir+'/'+x\n",
        "    print(x)\n",
        "    img=PIL.Image.open(x)\n",
        "    r=400/width\n",
        "    img=img.resize((int(width*r),int(height*r)))\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFVOj3EGna0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddEjvURICsRL",
        "outputId": "78985616-7482-47c1-f11c-5f460dcfb544"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'src.tools.original_control_net' from '/content/src/tools/original_control_net.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mwR2Fb97G6y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab018ae16a9b4fc3af629f587d80458a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_870d95156a0348c58d8e605564fa296f",
            "style": "IPY_MODEL_5cca6e8c78c54492a6ee048ef8413224",
            "tooltip": ""
          }
        },
        "870d95156a0348c58d8e605564fa296f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cca6e8c78c54492a6ee048ef8413224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}